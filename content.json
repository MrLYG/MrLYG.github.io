{"pages":[{"title":"404","text":"","link":"/404/index.html"},{"title":"about","text":"","link":"/about/index.html"},{"title":"contact","text":"","link":"/contact/index.html"},{"title":"categories","text":"","link":"/categories/index.html"},{"title":"friends","text":"","link":"/friends/index.html"},{"title":"tags","text":"","link":"/tags/index.html"}],"posts":[{"title":"内存空间的分配与回收","text":"内存的分配分为连续分配和非连续分配，所谓连续分配就是把一个进程的数据连续分配到内存中，非连续分配就是把进程分配到分散的内存中。 1. 连续分配管理方式连续分配有三种方式，每一种方式都是逐步演变行程的且有各自优点的。还是那句话所有事物的存在必然又因果！ 1.1. 单一连续分配1.1.1. 什么是单一连续分配在这种管理方式中，内存被分为两个区域：系统区和用户区。顾名思义系统区存放系统相关数据，用户区存放用户进程相关数据。内存中只能有一道用户程序，用户程序独占整个用户区空间。 在最初的PC系统中，并不支持多道程序运行，单一连续分配体现了其优点：实现简单；无外部碎片；可以采用覆盖技术扩充内存；不一定需要采取内存保护 但也有很明显的缺点：单一连续分配管理内存的方式仅用户单用户，单道程序的操作系统中；有内部碎片，存储器利用率低（想想一次只运行一个程序的机器，在程序大小远小于存储器容量大小的时候，利用率肯定低） 1.2. 固定分区分配1.2.1. 什么事固定分区分配内存分为系统去和用户区，固定分区分配是把用户区分为很多个更小的区，在每个分区只装入一个程序（最早的可运行的多道程序内存管理方式）。根据固定分区分配把每一个分区分的是否相等，固定分区分配又有两种情况1.分区大小相等 2.分区大小不相等 固定分区分配，分区大小相等 缺乏灵活性，但是很适合用于用一台计算机控制多个相同对象的场合（比如：钢铁厂有n个相同的炼钢炉，就可把内存分为n个大小相等的区域存放n个炼钢炉控制程序） 固定分区分配，分区大小不相等 增加了灵活性，可以满足不同大小的进程需求。根据常在系统中运行的作业大小情况进行划分（比如：划分多个小分区、适量中等分区、少量大分区） 1.2.2. 如何实现固定分区分配要实现对分区的管理，我们就需要设计某种数据结构来组织分区并存储分区的相关数据。操作系统提供了这种数据结构分区说明表（数组或者链表都可实现） 当某用户程序要装入内存时，由操作系统内核程序根据用户程序大小检索该表，从中找到一个能满足大小的、未分配的分区，将之分配给该程序，然后修改状态为“已分配”。优点：实现简单，无外部碎片。若一个用户程序（进程）的大小，超出内存所剩分区中最大容量的分区，进程也可放入该分区，不过需要使用覆盖技术进行逻辑扩容。缺点：a. 当用户程序太大时，可能所有的分区都不能满足需求，此时不得不采用覆盖技术来解决，但这又会降低性能；b. 会产生内部碎片，内存利用率低。 1.3. 动态分区分配1.3.1. 为什么要有动态分区分配固定分区分配内容管理方式会出现所有分区都不能满足程序大小的情况，且使用覆盖技术解决会造成性能降低。有没有一种内存管理方式可以给每一个程序放入内存时，自动分配空间大小，而不是提前把内存划分好？有的，就是动态分区分配，动态分区分配算法解决的主要矛盾就是程序的大小是无法预知的以至于无法按照实际预想装入固定的内存分区中。 在动态分区分配管理下的进程装入的简单流程： 1.3.2. 动态分区分配的概念动态分区分配不会预先划分内存分区，需要有一种数据结构来存储内存中的空闲分区和已分配的情况，在程序装入内存时，根据进程的大小动态地建立分区。 优点：动态分区分配没有内部碎片，但是有外部碎片 内部碎片，分配给某进程的内存区域中，如果有些部分没有用上。外部碎片，是指内存中的某些空闲分区由于太小而难以利用。 为何动态分区没有内部碎片，但有外部碎片？ 没有内部碎片——因为内存中分区的大小是根据进程大小自动分配的，所以不存在在某个分区内有未被使用的内存； 有外部碎片—— 因为在连续分配管理模式下，进程的内存是连续分配的，在进程结束替换进更小的进程后，导致两进程间出现很小的内存区域，小到无法装入任何一个进程。 如何解决外部碎片？ 如果内存中空闲空间的总和本来可以满足某进程的要求，但由于进程需要的是一整块连续的内存空间，因此这些“碎片”不能满足进程的需求。可以通过紧凑（拼凑，Compaction）技术来解决外部碎片。 1.3.3. 动态分区分配的数据结构还记得分区说明表吗？分区说明表是帮助操作系统实现固定分区分配的一种数据结构。实现动态分区分配使用的数据结构是空闲分区表(空闲分区链) 空闲分区表：每个空闲分区对应一个表项。表项中包含分区号、分区大小、分区起始地址等信息。 空闲分区链：每个分区的起始部分和末尾部分分别设置前向指针和后向指针。起始部分处还可记录分区大小等信息。 1.3.4. 动态分区分配管理方式的分配算法在动态分区分配方式中， 当很多个空闲分区都能满足需求时，应该选择哪个分区进行分配？ 最佳适应算法(Best Fit) 何为最佳呢？ 大进程进入内存是能有连续的空间进行存储。 把空闲分区的表项按照分区大小进行从小到大的排序，每次都从头的开始找，找到第一个能满足大小的空闲分区。这样的话可以尽可能的保留大片的空闲区，给大进程使用。 缺点：会造成很多细小的、难以利用的小块内存块，产生大量的外部碎片。 最坏适应算法(worst Fit) 为了解决最佳适应算法的问题——即留下太多难以利用的小碎片，可以在每次分配时优先使用最大的连续空闲区，这样分配后剩余的空闲区就不会太小，更方便使用。 把空分区表的表项按照分区大小进行从大到小的排序，每次都从头的开始找，找到第一个能满足大小的空闲分区。 缺点：有大进程要进入内存时，因为Worst Fit 算法优先使用大块的分区分配给进程，可能会出现没有足够的连续内存存储大进程的情况。 首次适应算法(First Fit) 把空闲分区表的表项按照起始地址进行排序，每次都从地址低的开始找，找到第一个能满足大小的空闲分区。 缺点：法每次都从链头开始查找的。这可能会导致低地址部分出现很多小的空闲分区，而每次分配查找时，都要经过这些分区，因此也增加了查找的开销。 邻近使用算法(Next Fit) 解决首次适应算法，每次从低地址开始找导致的在低地址部分出现很多小的空闲分区，而且由于局部性原理，每次都从低地址开始找，效率低等问题。邻近使用算法(Next Fit)每次都从上次查找结束的位置开始检索。 缺点：虽然解决了低地址有更多的外部碎片问题，但是Next Fit 算法会导致在高地址部分也会被划分为小分区 综上介绍了四种动态分区分配算法，各有各的缺点，相比之下首次适应算法(First Fit)虽然查询效率有影响，且会在低地址部分产生更多的小分区。但是如果有大进程进入内存时，在高地址部分有大分区可以使用。 且不用进行排序。 1.3.5. 动态分区分配管理方式的回收概念性的东西，理解就好！ 回收区的后面有一个相邻的空闲分区 两个相邻的空闲分区合并为一个 回收区的前面有一个相邻的空闲分区 两个相邻的空闲分区合并为一个 回收区的前、后各有一个相邻的空闲分区 三个相邻的空闲分区合并为一个 回收区的前、后都没有相邻的空闲分区 新增一个空闲分区表的表项或者空闲分区链的节点 2. 非连续分配管理方式","link":"/2021/08/11/%E5%86%85%E5%AD%98%E7%A9%BA%E9%97%B4%E7%9A%84%E5%88%86%E9%85%8D%E4%B8%8E%E5%9B%9E%E6%94%B6%20df4a3ea1a0ce48f9a179b09dafc9a32d/"},{"title":"Cache","text":"缓存意义万物皆有存在的意义，意义更多时候是弥补现有的不足。 Cache的意义在于弥补现有存储体系数据读写速度和CPU运行速度的鸿沟。简单来说就是CPU运算的速度远远快于从CPU从内存中读取数据的时间，添加缓存弥补时间上的差距，整体提高系统性能。 局部性原理何为局部性原理，局部性原理从何而来，他的意义是什么。我的理解是局部性原理是一种经验性的发现，一种结论，计算机的很多体系设置都是基于这个发现的。 来看一下CSAPP上关于Locality的介绍 Well-written computer programs tend to exhibit good locality. That is, they tend to reference data items that are near other recently referenced data items or that were recently referenced themselves. This tendency, known as the principle of locality, is an enduring concept that has enormous impact on the design and performance of hardware and software systems.一个编写良好的计算机程序常常具有良好的局部性(locality)。 也就是,它们倾向于引用邻近于其他最近引用过的数据项的数据项,或者最近引用过的数据项本身。这种倾向性,被称为局部性原理(principleof locality), 是一个持久的概念,对硬件和软件系统的设计和性能都有着极大的影响。 Locality is typically described as having two distinct forms: temporal locality and spatial locality. In a program with good temporal locality, a memory location that is referenced once is likely to be referenced again multiple times in the near future. In a program with good spatial locality, if a memory location is referenced局部性通常有两种不同的形式:时间局部性(temporal locality)和空间局部性(spalial locallty)。 在一个具有良好时间局部性的程序中,被引用过一次的内存位置很可能在不远的将来再被多次引用。在一个具有良好空间局部性的程序中,如果一个内存位置被引用了一次,那么程序很可能在不远的将来引用附近的一个内存位置。 什么是局部性局部性有两种形式 1.时间局部性 被引用过一次的位置未来会被多次引用 2.空间局部性 存储器中被某一数据的位置被引用，那么将来该位置附近的位置也会被引用 通过一个简单的CSAPP书上的例子解释一下，时间局部性和空间局部性 1234567int sumvec(int v[N]){ int i, sum = 0; for (i = 0; i &lt; N; i++) sum += v[i]; return sum;} 如图数组v的元素是被顺序读取，所以对于变量v, 他有很好的空间局部性（因为在V0被引用以后，v0位置附近的位置也会被引用)，但数组变量v的时间局部性很差，因为v数组中每个数据只引用一次。变量sum有很好的的时间局部性，因为他被引用后会被多次引用。 具有良好局部性的代码，CPU在连续访问数据时，每次访问的数据的在存储空间位置上的步长更短，最好的情况就是被访问的数据时连续且依次被CPU访问的。 步长和缓存命中也有关系 局部性原理和缓存的关系缓存的主要目的是为了解决CPU和一般存储器间速度的鸿沟，缓存符合局部性原理。 在计算机存储体系中上层存储器是下层存储器的缓存。下层存储器中的数据一旦被装入上层存储器，就很有可能被多次用到，此时CPU则可以在更靠近它的性能也更高的存储器内找到其需要的数据，而不需要每次都要到离它更远，性能也更低的存储器内去存取它们[1]。 设想我们很有钱买了一台所有存储器材料SRAM（寄存器所使用的材料，读写速度极快）的计算机，这样我们的CPU即使不需要缓存也能很快的去访问数据。但是仔细想一想这样是造成了极大的资源浪费的，因为在一个时刻，存储器中的数据只有一部分是被引用的（局部性原理），而存储器中其他的大部分数据都是空闲的，那么这些空闲的数据还是存到SRAM为材料的存储器中的话会很浪费资源。 在某一时刻，上图有色区域是CPU所要使用的数据，其他的位置都是空闲的，这样会有极大的浪费。———即使所有的数据直接存储到很大的寄存器中，但同一时刻，你可能只会用到其中的一部分。所以还不如用缓存架构。 总有些是正在工作，总有些是空闲的。缓存设计的理念就是将工作的数据抽出来，不工作的数据还留在原有的存储器中。 如上图，简单抽象缓存的设计思路（实际上要复杂很多），某一进程启动时先将将磁盘的数据先加载到内存中，在将数据加载到缓存中。 这样的好处 1.解决了CPU运算很快，数据读写很慢的情况。 2.因为读写越快的存储器越贵，将工作的数据抽到缓存中，暂时不工作的数据存放在稍便宜的但读写偏慢的存储器中（内存），将资源性文件，例如一个视频文档等，在不用的时候放到磁盘中，这样资源的利用率高。 数据分块的概念知道了为何有缓存以及缓存的设计理念后，我们可以想到把CPU目前访问的地址和周围的部分数据放到缓存中会极大的提高程序效率。 这就引出了数据分块的概念，将内存的存储空间进行“分块”，主存于缓存Cache之间以“块”为单位进行数据交换。 Core i7中所有cache的块大小都是64B[2] 为了更好地理解，内存的大小为8MB，Cache大小为8KB（具体如图所示） 由此我们来思考几个问题：1.内存的块和Cache的块如何对应呢？ 2. 因为Cache的存储容量是远小于内存容量的，那么Cache容量满了之后应该如何处理呢？3.CPU修改Cache中的数据后，如何同步到内存中，以及在并发场景下如何保证数据的一致性呢？ Cache和内存的3种映射关系CPU根据地址寻找数据时总是先在缓存中寻找，在缓存中没有找到（缓存未命中）后才会去内存寻找，然后，根据局部性原理，把CPU所寻找的数据所在位置的数据块加载到缓存，使CPU在下次寻找数据时直接能在缓存中找到。 还是以内存的大小为8MB，Cache大小为8KB为例—- cahce和内存的映射关系 全相联映射直接映射组相联映射Cache中数据的替换算法cache 满了怎么办 —替换算法 有四种方法分别是 随机算法(RAND) 先进先出算法(FIFO) 近期最少使用算法(LRU 面试高频) 最近不经常使用算法(LFU) 随机算法(RAND)随机算法（RAND, Random）——若Cache已满，则随机选择一块替换。 实现简单，但完全没考虑局部性原理，命中率低，实际效果很不稳定 先进先出算法(FIFO)先进先出算法（FIFO, First In First Out）——若Cache已满，则替换最先被调入Cache 的块 先进先出算法——实现简单，最开始按#0#1#2#3放入Cache，之后轮流替换 #0#1#2#3FIFO依然没考虑局部性原理，最先被调入Cache的块也有可能是被频繁访问的 ✨近期最少使用算法(LRU)近期最少使用算法（LRU, Least Recently Used ）—— 为每一个Cache块设置一个“计数器”，用于记录每个Cache块已经有多久没被访问了。当Cache满后替换“计数器”最大的 LRU实现过程①命中时，所命中的行的计数器清零，比其低的计数器加1，其余不变 → 当一个Cache有2^n个数据块时，计数器只需要n位当数据满时，加入新的数据块，计数器的值一定是不重复的。②未命中且还有空闲行时，新装入的行的计数器置0，其余非空闲行全加1；③未命中且无空闲行时，计数值最大的行的信息块被淘汰，新装行的块的计数器置0，其余全加1。 LRU算法缓存淘汰算法–LRU算法 其实吧，LRU也就那么回事。 自己实现了一个读数据和插入数据以及删除数据都为O(1)的数据结构，是HashMap和链表的结合，HashMap优化查询，链表优化插入和删除。Java有自带的LinkedHashMap的库； https://gist.github.com/MrLYG/38f0a461c24fefcb4dbea26e3e32f537 https://replit.com/@kenli5/StudyJavaToMaster#LeetCode/src/main/java/LRU/LRUCache.java LRU Pros&amp;Conspros→LRU算法——基于“局部性原理”，近期被访问过的主存块，在不久的将来也很有可能被再次访问，因此淘汰最久没被访问过的块是合理的。LRU算法的实际运行效果优秀，Cache命中率高。 cons→若被频繁访问的主存块数量 &gt; Cache行的数量，则有可能发生“抖动”，如：{1,2,3,4,5,1,2,3,4,5,1,2…} cache 只有四组的情况 解释一下： 当cache只有四组的情况时 如图：红色代表当次访问的到数据块从内存加载到了cache中 可以发现在这种情况中，LRU算法退化成了FIFO算法，导致效率低下，且缓存命中率为0，跟没加缓存的性能一样。所以如果出现程序突然的运行效率变低，需要注意是否是出现抖动的情况； 如何修复抖动情况： 避免CPU访问5号位的数据。 相信大家看到这里，跟我第一次学的时候一样的懵B无措，希望大家带着问题继续往下看，下面将讲解到底什么是“抖动”。 抖动(thrash)The term thrashing describes any situation where a cache is repeatedly loading and evicting the same sets of cache blocks.术语“抖动”描述了一种情况，即高速缓存重复的加载和删除相同的高速缓存块组 eg: 12345678float dotprod(float x [8], float y [8]){ float sum = 0.0; int i; for (i = 0; i &lt; 8; i++) sum += x [i] * y [i]; return sum;} Suppose that floats are 4 bytes, that x is loaded into the 32 bytes of contiguous memory starting at address 0, and that y starts immediately after x at address 32. For simplicity, suppose that a block is 16 bytes (big enough to hold four floats) and that the cache consists of two sets, for a total cache size of 32 bytes. We will assume that the variable sum is actually stored in a CPU register and thus does not require a memory reference. Given these assumptions, each x[i] and y[i] will map to the identical cache set: At run time, the first iteration of the loop references x[0] , a miss that causes the block containing x[0]−x [3] to be loaded into set 0. The next reference is to y[0] , another miss that causes the block containing y [0]−y [3] to be copied into set 0, overwriting the values of x that were copied in by the previous reference. During the next iteration, the reference to x[1] misses, which causes the x[0]−x [3] block to be loaded back into set 0, overwriting the y[0]−y[3] block. So now we have a conflict miss, and in fact each subsequent reference to x and y will result in a conflict miss as we thrash back and forth between blocks of x and y . The term thrashing describes any situation where a cache is repeatedly loading and evicting the same sets of cache blocks. The bottom line is that even though the program has good spatial locality and we have room in the cache to hold the blocks for both x[i] and y[i] , each reference results in a conflict miss because the blocks map to the same cache set. It is not unusual for this kind of thrashing to result in a slowdown by a factor of 2 or 3. Also, be aware that even though our example is extremely simple, the problem is real for larger and more realistic direct-mapped caches. Luckily, thrashing is easy for programmers to fix once they recognize what is going on. One easy solution is to put B bytes of padding at the end of each array. For example, instead of defining x to be float x[8] , we define it to be float x[12] . Assuming y starts immediately after x in memory, we have the following mapping of array elements to sets: With the padding at the end of x, x[i] and y[i] now map to different sets, which eliminates the thrashing conflict misses. 最近不经常使用算法(LFU)最不经常使用算法（LFU, Least Frequently Used ）—— 为每一个Cache块设置一个“计数器”，用于记录每个Cache块被访问过几次。当Cache满后替换“计数器”最小的 LFU算法——曾经被经常访问的主存块在未来不一定会用到（如：微信视频聊天相关的块），并没有很好地遵循局部性原理，因此实际运行效果不如 LRU 写操作如何确保数据的一致性写命中写回法(write-back)— 当CPU对Cache写命中时，只修改Cache的内容，而不立即写入主存，只有当此块被换出时才写回主存 减少了访存次数，但存在数据不一致的隐患。 全写法(写直通法，write-through)当CPU对Cache写命中时，必须把数据同时写入Cache和主存，一般使用写缓冲(write buffer) 访存次数增加，速度变慢，但更能保证数据一致性； 使用写缓冲，CPU写的速度很快，若写操作不频繁，则效果很好。若写操作很频繁，可能会因为写缓冲饱和而发生阻塞 写不命中写分配法(write-allocate)当CPU对Cache写不命中时，把主存中的块调入Cache，在Cache中修改。通常搭配写回法使用。 非写分配法(not-write-allocate)当CPU对Cache写不命中时只写入主存，不调入Cache。搭配全写法使用。 多级Cache各级Cache之间常采用“全写法+非写分配法” Cache和主存之间采用“写回发+写分配法” 缓存命中 借鉴内容[1]https://www.cnblogs.com/feng9exe/p/6882414.html [2]https://www.huaweicloud.com/zhishi/arc-9558555.html","link":"/2021/08/11/Cahce%20309fdb1a9e5d41fbb410de41083a0785/"}],"tags":[{"name":"计算机组成","slug":"计算机组成","link":"/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90/"},{"name":"操作系统","slug":"操作系统","link":"/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"},{"name":"Cache","slug":"Cache","link":"/tags/Cache/"}],"categories":[]}